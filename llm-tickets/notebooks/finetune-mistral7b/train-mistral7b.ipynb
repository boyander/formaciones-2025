{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2efe6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyander/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tune Llama2-7b on SE paired dataset\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset,DatasetDict\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model_state_dict\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from random import randrange\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154456d",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c22700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'id', 'parsed_data', 'raw_data'],\n",
       "        num_rows: 2043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'id', 'parsed_data', 'raw_data'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'id', 'parsed_data', 'raw_data'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "#cord_dataset = load_dataset(\"mychen76/receipts_sroie_v2\")\n",
    "dataset = load_dataset(\"mychen76/invoices-and-receipts_ocr_v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130ea656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 2043\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "print(f\"dataset size: {len(dataset['train'])}\")\n",
    "sample=dataset['train'][randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3091b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029 8122\n",
      "['Invoice no: 61356291', 'Date of issue:', '09/06/2012', 'Client:', 'Seller:', 'Chapman, Kim and Green', 'Rodriguez-Stevens', '64731 James Branch', '2280 Angela Plain', 'Smithmouth, NC 26872', 'Hortonshire, MS 93248', 'Tax Id: 949-84-9105', 'Tax Id: 939-98-8477', 'IBAN: GB50ACIE59715038217063', 'ITEMS', 'No.', 'Description', 'Qty', 'UM', 'Net price', 'Net worth', 'VAT [%]', 'Gross', ' worth', 'Wine Glasses Goblets Pair Clear', '12,00', '60,00', '10%', '5,00', 'each', '66,00', '1.', 'Glass', 'With Hooks Stemware Storage', '28,08', '112,32', '10%', '4,00', 'each', '123,55', 'Multiple Uses Iron Wine Rack', 'Hanging Glass', '1,00', '7,50', '7,50', '10%', '8,25', 'Replacement Corkscrew Parts', 'each', 'Spiral Worm Wine Opener Bottle', 'Houdini', 'HOME ESSENTIALS GRADIENT', '1,00', 'each', '12,99', '12,99', '10%', '14,29', '4', 'STEMLESS WINE GLASSES SET', 'OF 4 20 FL OZ (591 ml) NEW', 'SUMMARY', 'VAT [%]', 'Net worth', 'VAT', 'Gross worth', '10%', '192,81', '19,28', '212,09', 'Total', '$ 192,81', '$ 19,28', '$ 212,09']\n",
      "[[[[199.0, 110.0], [801.0, 110.0], [801.0, 161.0], [199.0, 161.0]], ('Invoice no: 61356291', 0.9788082242012024)], [[[196.0, 212.0], [517.0, 212.0], [517.0, 259.0], [196.0, 259.0]], ('Date of issue:', 0.9985820651054382)], [[[1200.0, 216.0], [1462.0, 216.0], [1462.0, 256.0], [1200.0, 256.0]], ('09/06/2012', 0.9996706247329712)], [[[1239.0, 653.0], [1426.0, 662.0], [1423.0, 724.0], [1236.0, 715.0]], ('Client:', 0.9993823170661926)], [[[192.0, 665.0], [388.0, 665.0], [388.0, 716.0], [192.0, 716.0]], ('Seller:', 0.9997318387031555)], [[[210.0, 756.0], [753.0, 756.0], [753.0, 808.0], [210.0, 808.0]], ('Chapman, Kim and Green', 0.9998903870582581)], [[[1248.0, 760.0], [1647.0, 760.0], [1647.0, 808.0], [1248.0, 808.0]], ('Rodriguez-Stevens', 0.9998601078987122)], [[[207.0, 807.0], [650.0, 811.0], [650.0, 863.0], [207.0, 859.0]], ('64731 James Branch', 0.9947491884231567)], [[[1248.0, 807.0], [1636.0, 811.0], [1635.0, 863.0], [1248.0, 859.0]], ('2280 Angela Plain', 0.9994451999664307)], [[[207.0, 866.0], [701.0, 866.0], [701.0, 914.0], [207.0, 914.0]], ('Smithmouth, NC 26872', 0.9997967481613159)], [[[1244.0, 866.0], [1732.0, 866.0], [1732.0, 914.0], [1244.0, 914.0]], ('Hortonshire, MS 93248', 0.9944106340408325)], [[[207.0, 972.0], [639.0, 972.0], [639.0, 1020.0], [207.0, 1020.0]], ('Tax Id: 949-84-9105', 0.9908804893493652)], [[[1248.0, 972.0], [1676.0, 972.0], [1676.0, 1020.0], [1248.0, 1020.0]], ('Tax Id: 939-98-8477', 0.9881197214126587)], [[[207.0, 1023.0], [923.0, 1023.0], [923.0, 1071.0], [207.0, 1071.0]], ('IBAN: GB50ACIE59715038217063', 0.9735023379325867)], [[[188.0, 1140.0], [377.0, 1140.0], [377.0, 1191.0], [188.0, 1191.0]], ('ITEMS', 0.973893940448761)], [[[233.0, 1268.0], [303.0, 1268.0], [303.0, 1308.0], [233.0, 1308.0]], ('No.', 0.9999094009399414)], [[[358.0, 1268.0], [576.0, 1268.0], [576.0, 1308.0], [358.0, 1308.0]], ('Description', 0.9997653961181641)], [[[1012.0, 1261.0], [1097.0, 1261.0], [1097.0, 1316.0], [1012.0, 1316.0]], ('Qty', 0.9998303055763245)], [[[1159.0, 1264.0], [1237.0, 1264.0], [1237.0, 1308.0], [1159.0, 1308.0]], ('UM', 0.9347040057182312)], [[[1348.0, 1268.0], [1528.0, 1268.0], [1528.0, 1308.0], [1348.0, 1308.0]], ('Net price', 0.9999516010284424)], [[[1580.0, 1268.0], [1780.0, 1268.0], [1780.0, 1308.0], [1580.0, 1308.0]], ('Net worth', 0.9986754655838013)], [[[1842.0, 1268.0], [2001.0, 1268.0], [2001.0, 1308.0], [1842.0, 1308.0]], ('VAT [%]', 0.9989665746688843)], [[[2134.0, 1261.0], [2259.0, 1261.0], [2259.0, 1312.0], [2134.0, 1312.0]], ('Gross', 0.9985858798027039)], [[[2129.0, 1296.0], [2261.0, 1306.0], [2257.0, 1357.0], [2125.0, 1347.0]], (' worth', 0.9336559772491455)], [[[351.0, 1388.0], [897.0, 1396.0], [897.0, 1444.0], [350.0, 1436.0]], ('Wine Glasses Goblets Pair Clear', 0.9984278678894043)], [[[1421.0, 1392.0], [1532.0, 1392.0], [1532.0, 1443.0], [1421.0, 1443.0]], ('12,00', 0.9996110200881958)], [[[1676.0, 1392.0], [1787.0, 1392.0], [1787.0, 1443.0], [1676.0, 1443.0]], ('60,00', 0.9979851841926575)], [[[1909.0, 1383.0], [2007.0, 1394.0], [2000.0, 1449.0], [1903.0, 1438.0]], ('10%', 0.9999048113822937)], [[[1004.0, 1396.0], [1097.0, 1396.0], [1097.0, 1440.0], [1004.0, 1440.0]], ('5,00', 0.9872055053710938)], [[[1148.0, 1400.0], [1248.0, 1400.0], [1248.0, 1440.0], [1148.0, 1440.0]], ('each', 0.9973973035812378)], [[[2145.0, 1392.0], [2259.0, 1392.0], [2259.0, 1443.0], [2145.0, 1443.0]], ('66,00', 0.9967435002326965)], [[[251.0, 1403.0], [284.0, 1403.0], [284.0, 1432.0], [251.0, 1432.0]], ('1.', 0.9957729578018188)], [[[353.0, 1435.0], [459.0, 1445.0], [455.0, 1485.0], [349.0, 1475.0]], ('Glass', 0.997931957244873)], [[[355.0, 1520.0], [872.0, 1528.0], [871.0, 1575.0], [354.0, 1567.0]], ('With Hooks Stemware Storage', 0.9999193549156189)], [[[1418.0, 1524.0], [1532.0, 1524.0], [1532.0, 1575.0], [1418.0, 1575.0]], ('28,08', 0.9997357130050659)], [[[1654.0, 1524.0], [1783.0, 1524.0], [1783.0, 1575.0], [1654.0, 1575.0]], ('112,32', 0.9998149871826172)], [[[1909.0, 1515.0], [2007.0, 1526.0], [2001.0, 1577.0], [1903.0, 1566.0]], ('10%', 0.9999029040336609)], [[[1008.0, 1527.0], [1093.0, 1527.0], [1093.0, 1571.0], [1008.0, 1571.0]], ('4,00', 0.9987012147903442)], [[[1152.0, 1527.0], [1244.0, 1527.0], [1244.0, 1571.0], [1152.0, 1571.0]], ('each', 0.9990001320838928)], [[[2130.0, 1527.0], [2252.0, 1527.0], [2252.0, 1568.0], [2130.0, 1568.0]], ('123,55', 0.9989001154899597)], [[[358.0, 1571.0], [849.0, 1571.0], [849.0, 1608.0], [358.0, 1608.0]], ('Multiple Uses Iron Wine Rack', 0.9708606600761414)], [[[358.0, 1615.0], [605.0, 1615.0], [605.0, 1655.0], [358.0, 1655.0]], ('Hanging Glass', 0.9997545480728149)], [[[1008.0, 1696.0], [1093.0, 1696.0], [1093.0, 1739.0], [1008.0, 1739.0]], ('1,00', 0.9919418096542358)], [[[1440.0, 1692.0], [1532.0, 1692.0], [1532.0, 1747.0], [1440.0, 1747.0]], ('7,50', 0.9992889165878296)], [[[1695.0, 1692.0], [1787.0, 1692.0], [1787.0, 1747.0], [1695.0, 1747.0]], ('7,50', 0.9993087649345398)], [[[1912.0, 1692.0], [2005.0, 1692.0], [2005.0, 1747.0], [1912.0, 1747.0]], ('10%', 0.9999165534973145)], [[[2168.0, 1687.0], [2254.0, 1697.0], [2247.0, 1752.0], [2161.0, 1741.0]], ('8,25', 0.9991562366485596)], [[[354.0, 1703.0], [864.0, 1703.0], [864.0, 1739.0], [354.0, 1739.0]], ('Replacement Corkscrew Parts', 0.9861046075820923)], [[[1152.0, 1699.0], [1244.0, 1699.0], [1244.0, 1743.0], [1152.0, 1743.0]], ('each', 0.9991188645362854)], [[[355.0, 1736.0], [901.0, 1739.0], [901.0, 1787.0], [354.0, 1783.0]], ('Spiral Worm Wine Opener Bottle', 0.9984367489814758)], [[[354.0, 1787.0], [495.0, 1787.0], [495.0, 1823.0], [354.0, 1823.0]], ('Houdini', 0.9994716644287109)], [[[351.0, 1867.0], [864.0, 1867.0], [864.0, 1915.0], [351.0, 1915.0]], ('HOME ESSENTIALS GRADIENT', 0.9798102378845215)], [[[1008.0, 1871.0], [1093.0, 1871.0], [1093.0, 1911.0], [1008.0, 1911.0]], ('1,00', 0.9990050792694092)], [[[1152.0, 1875.0], [1248.0, 1875.0], [1248.0, 1915.0], [1152.0, 1915.0]], ('each', 0.9982025623321533)], [[[1421.0, 1867.0], [1532.0, 1867.0], [1532.0, 1918.0], [1421.0, 1918.0]], ('12,99', 0.9998315572738647)], [[[1676.0, 1867.0], [1787.0, 1867.0], [1787.0, 1918.0], [1676.0, 1918.0]], ('12,99', 0.9997693300247192)], [[[1912.0, 1871.0], [2001.0, 1871.0], [2001.0, 1915.0], [1912.0, 1915.0]], ('10%', 0.9995101094245911)], [[[2149.0, 1871.0], [2256.0, 1871.0], [2256.0, 1911.0], [2149.0, 1911.0]], ('14,29', 0.9969544410705566)], [[[262.0, 1882.0], [281.0, 1882.0], [281.0, 1900.0], [262.0, 1900.0]], ('4', 0.9126366972923279)], [[[362.0, 1915.0], [864.0, 1915.0], [864.0, 1951.0], [362.0, 1951.0]], ('STEMLESS WINE GLASSES SET', 0.9976090788841248)], [[[354.0, 1948.0], [845.0, 1948.0], [845.0, 1995.0], [354.0, 1995.0]], ('OF 4 20 FL OZ (591 ml) NEW', 0.9766821265220642)], [[[192.0, 2094.0], [491.0, 2094.0], [491.0, 2145.0], [192.0, 2145.0]], ('SUMMARY', 0.9961081743240356)], [[[956.0, 2218.0], [1122.0, 2218.0], [1122.0, 2269.0], [956.0, 2269.0]], ('VAT [%]', 0.9993457794189453)], [[[1388.0, 2222.0], [1588.0, 2222.0], [1588.0, 2262.0], [1388.0, 2262.0]], ('Net worth', 0.9923312067985535)], [[[1775.0, 2213.0], [1870.0, 2223.0], [1865.0, 2267.0], [1771.0, 2257.0]], ('VAT', 0.672061026096344)], [[[2027.0, 2222.0], [2259.0, 2222.0], [2259.0, 2262.0], [2027.0, 2262.0]], ('Gross worth', 0.9999037384986877)], [[[997.0, 2309.0], [1082.0, 2309.0], [1082.0, 2353.0], [997.0, 2353.0]], ('10%', 0.9998807907104492)], [[[1458.0, 2306.0], [1591.0, 2306.0], [1591.0, 2357.0], [1458.0, 2357.0]], ('192,81', 0.9995806217193604)], [[[1757.0, 2306.0], [1868.0, 2306.0], [1868.0, 2357.0], [1757.0, 2357.0]], ('19,28', 0.9996465444564819)], [[[2134.0, 2309.0], [2263.0, 2309.0], [2263.0, 2350.0], [2134.0, 2350.0]], ('212,09', 0.9867501854896545)], [[[746.0, 2393.0], [857.0, 2393.0], [857.0, 2445.0], [746.0, 2445.0]], ('Total', 0.9998254776000977)], [[[1410.0, 2393.0], [1588.0, 2393.0], [1588.0, 2445.0], [1410.0, 2445.0]], ('$ 192,81', 0.9970046281814575)], [[[1709.0, 2393.0], [1868.0, 2393.0], [1868.0, 2445.0], [1709.0, 2445.0]], ('$ 19,28', 0.9348526000976562)], [[[2082.0, 2393.0], [2263.0, 2393.0], [2263.0, 2445.0], [2082.0, 2445.0]], ('$ 212,09', 0.9943768978118896)]]\n"
     ]
    }
   ],
   "source": [
    "ocr_words=eval(sample['raw_data'])['ocr_words']\n",
    "ocr_boxes=eval(sample['raw_data'])['ocr_boxes']\n",
    "print(len(ocr_words), len(ocr_boxes))\n",
    "print(ocr_words)\n",
    "print(ocr_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689be079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1238,\n",
       " \"{'header': {'invoice_no': '61356291', 'invoice_date': '09/06/2012', 'seller': 'Chapman, Kim and Green 64731 James Branch Smithmouth, NC 26872', 'client': 'Rodriguez-Stevens 2280 Angela Plain Hortonshire, MS 93248', 'seller_tax_id': '949-84-9105', 'client_tax_id': '939-98-8477', 'iban': 'GB50ACIE59715038217063'}, 'items': [{'item_desc': 'Wine Glasses Goblets Pair Clear Glass', 'item_qty': '5,00', 'item_net_price': '12,00', 'item_net_worth': '60,00', 'item_vat': '10%', 'item_gross_worth': '66,00'}, {'item_desc': 'With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass', 'item_qty': '4,00', 'item_net_price': '28,08', 'total_net_worth': '112,32', 'item_vat': '10%', 'item_gross_worth': '123,55'}, {'item_desc': 'Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini', 'item_qty': '1,00', 'item_net_price': '7,50', 'total_net_worth': '7,50', 'item_vat': '10%', 'item_gross_worth': '8,25'}, {'item_desc': 'HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 420 FL OZ (591 ml) NEW', 'item_qty': '1,00', 'item_net_price': '12,99', 'item_net_worth': '12,99', 'item_vat': '10%', 'item_gross_worth': '14,29'}], 'summary': {'total_net_worth': '$ 192,81', 'total_vat': '$19,28', 'total_gross_worth': '$ 212,09'}}\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_json=eval(sample['parsed_data'])['json']\n",
    "len(ocr_json), ocr_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14afba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_instruction(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "You are POS receipt expert, and receipt data engineer with many years on working with complex receipt structure. \n",
    "I need you parse, detect, recognize and convert following receipt OCR image result into structure receipt format. \n",
    "the outout mus be a well-formed json object.```json\n",
    "\n",
    "### Input:\n",
    "{eval(sample['raw_data'])['ocr_boxes']}\n",
    "\n",
    "### Output:\n",
    "{eval(sample['parsed_data'])['json']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744ad3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are POS receipt expert, and receipt data engineer with many years on working with complex receipt structure. \n",
      "I need you parse, detect, recognize and convert following receipt OCR image result into structure receipt format. \n",
      "the outout mus be a well-formed json object.```json\n",
      "\n",
      "### Input:\n",
      "[[[[196.0, 110.0], [801.0, 110.0], [801.0, 161.0], [196.0, 161.0]], ('Invoice no: 40378170', 0.9985853433609009)], [[[196.0, 212.0], [517.0, 212.0], [517.0, 259.0], [196.0, 259.0]], ('Date of issue:', 0.9883247017860413)], [[[1204.0, 216.0], [1466.0, 216.0], [1466.0, 256.0], [1204.0, 256.0]], ('10/15/2012', 0.9997990727424622)], [[[192.0, 665.0], [388.0, 665.0], [388.0, 716.0], [192.0, 716.0]], ('Seller:', 0.9997318387031555)], [[[1240.0, 661.0], [1429.0, 661.0], [1429.0, 720.0], [1240.0, 720.0]], ('Client:', 0.9994950890541077)], [[[203.0, 753.0], [931.0, 760.0], [930.0, 811.0], [203.0, 804.0]], ('Patel, Thompson and Montgomery', 0.9964486360549927)], [[[1244.0, 756.0], [1890.0, 756.0], [1890.0, 808.0], [1244.0, 808.0]], ('Jackson, Odonnell and Jackson.', 0.9805290102958679)], [[[207.0, 807.0], [513.0, 811.0], [513.0, 863.0], [207.0, 859.0]], ('356 Kyle Vista', 0.9985752105712891)], [[[1244.0, 807.0], [1776.0, 811.0], [1776.0, 863.0], [1244.0, 859.0]], ('267 John Track Suite 841', 0.9817495346069336)], [[[207.0, 866.0], [679.0, 866.0], [679.0, 914.0], [207.0, 914.0]], ('New James, MA 46228', 0.9982457160949707)], [[[1244.0, 866.0], [1724.0, 866.0], [1724.0, 914.0], [1244.0, 914.0]], ('Jenniferville, PA 98601', 0.9854620099067688)], [[[207.0, 972.0], [635.0, 972.0], [635.0, 1020.0], [207.0, 1020.0]], ('Tax Id: 958-74-3511', 0.9896508455276489)], [[[1248.0, 972.0], [1680.0, 972.0], [1680.0, 1020.0], [1248.0, 1020.0]], ('Tax Id: 998-87-7723', 0.9801385998725891)], [[[203.0, 1019.0], [960.0, 1023.0], [960.0, 1074.0], [203.0, 1071.0]], ('IBAN: GB77WRBQ31965128414006', 0.9939054250717163)], [[[192.0, 1140.0], [377.0, 1140.0], [377.0, 1191.0], [192.0, 1191.0]], ('ITEMS', 0.9929741621017456)], [[[1161.0, 1252.0], [1239.0, 1262.0], [1232.0, 1313.0], [1154.0, 1303.0]], ('UM', 0.979519248008728)], [[[233.0, 1268.0], [303.0, 1268.0], [303.0, 1308.0], [233.0, 1308.0]], ('No.', 0.9999094009399414)], [[[358.0, 1268.0], [576.0, 1268.0], [576.0, 1308.0], [358.0, 1308.0]], ('Description', 0.9997653961181641)], [[[1015.0, 1264.0], [1093.0, 1264.0], [1093.0, 1316.0], [1015.0, 1316.0]], ('Qty', 0.9998195767402649)], [[[1348.0, 1268.0], [1528.0, 1268.0], [1528.0, 1308.0], [1348.0, 1308.0]], ('Net price', 0.9999516010284424)], [[[1580.0, 1268.0], [1780.0, 1268.0], [1780.0, 1308.0], [1580.0, 1308.0]], ('Net worth', 0.9986754655838013)], [[[1842.0, 1268.0], [2001.0, 1268.0], [2001.0, 1308.0], [1842.0, 1308.0]], ('VAT [%]', 0.9989665746688843)], [[[2134.0, 1261.0], [2259.0, 1261.0], [2259.0, 1312.0], [2134.0, 1312.0]], ('Gross', 0.9985858798027039)], [[[2130.0, 1305.0], [2259.0, 1305.0], [2259.0, 1356.0], [2130.0, 1356.0]], ('worth', 0.9997998476028442)], [[[351.0, 1396.0], [875.0, 1396.0], [875.0, 1443.0], [351.0, 1443.0]], (\" Leed's Wine Companion Bottle\", 0.9855892658233643)], [[[1008.0, 1396.0], [1097.0, 1396.0], [1097.0, 1440.0], [1008.0, 1440.0]], ('1,00', 0.9966188669204712)], [[[1148.0, 1400.0], [1248.0, 1400.0], [1248.0, 1440.0], [1148.0, 1440.0]], ('each', 0.9973973035812378)], [[[1444.0, 1396.0], [1528.0, 1396.0], [1528.0, 1440.0], [1444.0, 1440.0]], ('7,50', 0.9954859018325806)], [[[1698.0, 1396.0], [1783.0, 1396.0], [1783.0, 1440.0], [1698.0, 1440.0]], ('7,50', 0.9905357360839844)], [[[1916.0, 1396.0], [2001.0, 1396.0], [2001.0, 1440.0], [1916.0, 1440.0]], ('10%', 0.9995570182800293)], [[[2171.0, 1396.0], [2256.0, 1396.0], [2256.0, 1440.0], [2171.0, 1440.0]], ('8,25', 0.9729064106941223)], [[[355.0, 1432.0], [875.0, 1436.0], [875.0, 1484.0], [354.0, 1480.0]], ('Corkscrew Opener Gift Box Set', 0.9943951964378357)], [[[358.0, 1484.0], [613.0, 1484.0], [613.0, 1520.0], [358.0, 1520.0]], ('with Foil Cutter', 0.9756938219070435)], [[[196.0, 1619.0], [491.0, 1619.0], [491.0, 1670.0], [196.0, 1670.0]], ('SUMMARY', 0.9958321452140808)], [[[956.0, 1743.0], [1122.0, 1743.0], [1122.0, 1794.0], [956.0, 1794.0]], ('VAT [%]', 0.9992411732673645)], [[[1776.0, 1734.0], [1874.0, 1745.0], [1868.0, 1799.0], [1770.0, 1789.0]], ('VAT', 0.9691691398620605)], [[[1388.0, 1747.0], [1588.0, 1747.0], [1588.0, 1787.0], [1388.0, 1787.0]], ('Net worth', 0.9879683256149292)], [[[2023.0, 1750.0], [2256.0, 1750.0], [2256.0, 1787.0], [2023.0, 1787.0]], ('Gross worth', 0.9999182224273682)], [[[1500.0, 1822.0], [1593.0, 1833.0], [1587.0, 1887.0], [1493.0, 1876.0]], ('7,50', 0.9980185627937317)], [[[997.0, 1834.0], [1082.0, 1834.0], [1082.0, 1878.0], [997.0, 1878.0]], ('10%', 0.9996476173400879)], [[[1783.0, 1834.0], [1868.0, 1834.0], [1868.0, 1878.0], [1783.0, 1878.0]], ('0,75', 0.9830088019371033)], [[[2175.0, 1834.0], [2263.0, 1834.0], [2263.0, 1878.0], [2175.0, 1878.0]], ('8,25', 0.9898556470870972)], [[[746.0, 1918.0], [857.0, 1918.0], [857.0, 1970.0], [746.0, 1970.0]], ('Total', 0.9997448921203613)], [[[1455.0, 1918.0], [1591.0, 1918.0], [1591.0, 1970.0], [1455.0, 1970.0]], ('$7,50', 0.998517632484436)], [[[1735.0, 1918.0], [1868.0, 1918.0], [1868.0, 1970.0], [1735.0, 1970.0]], ('$ 0,75', 0.9555268883705139)], [[[2133.0, 1910.0], [2264.0, 1920.0], [2260.0, 1974.0], [2129.0, 1965.0]], ('$8,25', 0.9991155862808228)]]\n",
      "\n",
      "### Output:\n",
      "{'header': {'invoice_no': '40378170', 'invoice_date': '10/15/2012', 'seller': 'Patel, Thompson and Montgomery 356 Kyle Vista New James, MA 46228', 'client': 'Jackson, Odonnell and Jackson 267 John Track Suite 841 Jenniferville, PA 98601', 'seller_tax_id': '958-74-3511', 'client_tax_id': '998-87-7723', 'iban': 'GB77WRBQ31965128414006'}, 'items': [{'item_desc': \"Leed's Wine Companion Bottle Corkscrew Opener Gift Box Set with Foil Cutter\", 'item_qty': '1,00', 'item_net_price': '7,50', 'item_net_worth': '7,50', 'item_vat': '10%', 'item_gross_worth': '8,25'}], 'summary': {'total_net_worth': '$7,50', 'total_vat': '$0,75', 'total_gross_worth': '$8,25'}}\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "print(format_train_instruction(dataset[\"train\"][randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf5263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = format_train_instruction(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1297a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id=\"mychen76/invoices-and-receipts_ocr_v1\"\n",
    "data_dir=\"data/finetune\"\n",
    "\n",
    "def create_datasets(tokenizer,dataset_id,data_dir=None,seq_length=2048,num_workers=6,streaming=False,size_valid_set=10,shuffle_buffer=1000):\n",
    "    dataset = load_dataset(\n",
    "        dataset_id,\n",
    "        data_dir=data_dir,\n",
    "        split=\"train\",\n",
    "        token=True,\n",
    "        num_proc=num_workers if not streaming else None,\n",
    "        streaming=streaming,\n",
    "    )\n",
    "    if streaming:\n",
    "        print(\"Loading the dataset in streaming mode\")\n",
    "        valid_data = dataset.take(size_valid_set)\n",
    "        train_data = dataset.skip(size_valid_set)\n",
    "        train_data = train_data.shuffle(buffer_size=shuffle_buffer, seed=None)\n",
    "    else:\n",
    "        dataset = dataset.train_test_split(test_size=0.003, seed=None)\n",
    "        train_data = dataset[\"train\"]\n",
    "        valid_data = dataset[\"test\"]\n",
    "        print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
    "\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer)\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=format_train_instruction,\n",
    "        infinite=True,\n",
    "        seq_length=seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        formatting_func=format_train_instruction,\n",
    "        infinite=False,\n",
    "        seq_length=seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88988b6d",
   "metadata": {},
   "source": [
    "## load Model for training\n",
    "`torch.device('cuda:0')` Note that you don’t need to pass device_map when loading the model for training. It will automatically load your model on your GPU. You can also set the device map to a specific device if needed (e.g. cuda:0, 0, torch.device('cuda:0')). Please note that device_map=auto should be used for inference only.\n",
    "\n",
    "\n",
    "> Nota:\n",
    "> Login con hugging face antes `huggingface-cli login` \n",
    "> Instalar cli -> https://huggingface.co/docs/huggingface_hub/guides/cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab22184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e70ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "# Hugging Face model id\n",
    "#model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated \"meta-llama/Llama-2-7b-hf\n",
    "#model_id=\"PY007/TinyLlama-1.1B-intermediate-step-240k-503b\"\n",
    "model_id=\"TinyLlama/TinyLlama_v1.1\"\n",
    "#model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "#model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    #load_in_4bit=True,\n",
    "    #load_in_8bit=True,\n",
    "    #bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "##quantization_config=bnb_config, \n",
    "# Load model and tokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            #quantization_config=bnb_config,  \n",
    "                                             #use_auth_token=True,\n",
    "                                             token=True,\n",
    "                                             trust_remote_code=True,                                                  \n",
    "                                             device_map=\"mps\")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612936d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1100048384 || all params: 1100048384 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f9a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "## frezee the model\n",
    "for param in model_8bit.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model_8bit.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model_8bit.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model_8bit.lm_head = CastOutputToFloat(model_8bit.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddf21e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): CastOutputToFloat(\n",
       "    (0): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_8bit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2878741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.097999691963196 GB\n"
     ]
    }
   ],
   "source": [
    "print(model_8bit.get_memory_footprint()/1024/1024/1024, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd03f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(model_8bit.config.max_position_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995842a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyander/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 9,011,200 || all params: 1,109,059,584 || trainable%: 0.8125\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # skip this time\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "## prepare model for training\n",
    "#model = prepare_model_for_kbit_training(model_8bit)\n",
    "base_model = get_peft_model(model_8bit, peft_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82bd3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 2036. Size of the validation set: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:01<00:00, 206.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character to token ratio of the dataset is: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import transformers \n",
    "\n",
    "OUTPUT_DIR = \"./results/mistral7b_ocr_to_json\"\n",
    "NUM_TRAIN_EPOCHS=3\n",
    "SAVE_STEPS=20\n",
    "LOGGING_STEPS=10\n",
    "LEARNING_RATE=2e-4 #3e-4\n",
    "TRAIN_STEPS=150  #300\n",
    "#WARM_UP_STEPS=50  or ratio \n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,        \n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,        \n",
    "    optim=\"paged_adamw_32bit\",  \n",
    "    logging_steps=LOGGING_STEPS,    \n",
    "    save_total_limit=2,  \n",
    "    save_strategy=\"epoch\",    \n",
    "    learning_rate=2e-4,            ## LEARNING_RATE,    \n",
    "    #bf16=True,\n",
    "    #tf32=True,        \n",
    "    #bf16_full_eval=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,             ## warmup_steps=WARM_UP_STEPS,    \n",
    "    lr_scheduler_type=\"constant\",  ##\"cosine\"   \n",
    "    disable_tqdm=False,              # disable tqdm since with packing values are in correct    \n",
    "    #max_steps=TRAIN_STEPS,\n",
    "    report_to=\"tensorboard\",\n",
    "    #save_steps=SAVE_STEPS,\n",
    "    group_by_length=False,\n",
    "    #remove_unused_columns=False,\n",
    "    #evaluation_strategy=\"epoch\",  #steps\n",
    "    run_name=\"sft_mistral7b_colorist\",\n",
    ")\n",
    "train_dataset, eval_dataset = create_datasets(base_tokenizer, dataset_id, seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e3b36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer,SFTConfig,DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    packing=True,\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c71b6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set model in training mode\n",
    "base_model.config.pretraining_tp = 1\n",
    "base_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "## pytorch optimization \n",
    "# old_state_dict = base_model.state_dict\n",
    "# base_model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(base_model, type(base_model)) \n",
    "base_model = torch.compile(base_model)\n",
    "\n",
    "# # Enable cuDNN auto-tuner - NVIDIA cuDNN supports many algorithms to compute a convolution. \n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cc56fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5364a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyander/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/boyander/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/utils/_device.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.25 GB, other allocations: 8.26 GB, max allowed: 20.40 GB). Tried to allocate 4.00 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:733\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:687\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[33;03mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[32m    685\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    686\u001b[39m mode = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m (loss, outputs) = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    691\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/peft/peft_model.py:1757\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1756\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1770\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    451\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:47\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:488\u001b[39m, in \u001b[36mcheckpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    485\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muse_reentrant=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m     gen = _checkpoint_without_reentrant_generator(\n\u001b[32m    491\u001b[39m         function, preserve, context_fn, determinism_check, debug, *args, **kwargs\n\u001b[32m    492\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:263\u001b[39m, in \u001b[36mCheckpointFunction.forward\u001b[39m\u001b[34m(ctx, run_function, preserve_rng_state, *args)\u001b[39m\n\u001b[32m    260\u001b[39m ctx.save_for_backward(*tensor_inputs)\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     outputs = \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:265\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    277\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KIRBIC/content-talleres-jornadas/.venv/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 8.25 GB, other allocations: 8.26 GB, max allowed: 20.40 GB). Tried to allocate 4.00 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "# save model\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd71a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec264a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
